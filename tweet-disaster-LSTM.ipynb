{"cells":[{"cell_type":"markdown","metadata":{"id":"yKUzgyQu-0BL"},"source":["<h1 style=\"text-align:center;\"><b>Easy Data Augmentation (EDA) for Text Classification with Disaster Tweets</b></h1>\n","\n","<h2 style=\"text-align:center;\"><b>Course: Introduction to Natural Language Processing</b></h2>\n","\n","<h3 style=\"text-align:center;\"><b>University of Trento, 2021-22</b></h3>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"A-O1_2Uv0T5r"},"source":["# 0. Instructions\n","\n","The notebook is divided into the following sections:\n","- **1. Setup** \n","    - ***1.1 Import libraries***: import the required libraries\n","    - ***1.2 Set random seed***: set the random seed for reproducibility\n","- **2. Dataset**\n","    - ***2.1 Load and prepare Disaster Tweets dataset***: load the dataset and prepare it for the experiments\n","    - ***2.2 Resampling***: resample the dataset  (optional)\n","- **3. Data Preprocessing**\n","    - ***3.1 Cleaning***: clean the tweets\n","    - ***3.2 EDA: Easy Data Augmentation***: apply EDA to the tweets (optional)\n","    - ***3.3 Tokenization and Padding***: tokenize and pad the tweets\n","    - ***3.4 GloVe Embeddings***: load the GloVe embeddings and prepare the embedding matrix for the RNN model\n","- **4. RNN with LSTM cells**\n","    - ***4.1 Define the RNN model***: define the RNN model\n","    - ***4.2 Train and evaluate the RNN model***\n","\n","Follow the instructions in the notebook to run the code. \n","Remember that if you want to resample the dataset, you have to run the cells in the ***Resampling*** section. Otherwise, you can skip them.\n","Remember that if you want to use EDAs, you have to run the cells in the ***EDA*** section. Otherwise, you can skip them."]},{"cell_type":"markdown","metadata":{"id":"qnbWlFP50T5s"},"source":["# 1. Setup"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"w73sbxUk0T5w"},"source":["# 1.1 Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-30T19:35:55.756496Z","iopub.status.busy":"2022-12-30T19:35:55.756078Z","iopub.status.idle":"2022-12-30T19:35:57.950574Z","shell.execute_reply":"2022-12-30T19:35:57.949112Z","shell.execute_reply.started":"2022-12-30T19:35:55.756409Z"},"id":"NOi92uvO-0Bj","outputId":"fffe4a36-f53b-40ad-ea6d-e24c75b9460d","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","textattack: Updating TextAttack package dependencies.\n","textattack: Downloading NLTK required packages.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package omw to /root/nltk_data...\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# utilities\n","import re\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import os\n","\n","# plotting\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","# nltk\n","import nltk\n","nltk.download('omw-1.4')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","\n","\n","# sklearn\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","# sklearn nlp\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","\n","# keras\n","import tensorflow as tf\n","\n","# utils\n","from utils import save_metrics, print_metrics, plot_confusion_matrix\n","\n","# preprocessing\n","from preprocessing import preprocess_data\n","\n","# data augmentation\n","from data_augmentation import augment_data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T_UKC_59-0Bu"},"source":["# 1.2 Set random seed and create useful folders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_22fk01-0By"},"outputs":[],"source":["def set_seed(seed = 16):\n","    np.random.seed(16)\n","    tf.random.set_seed(16)\n","\n","set_seed()\n","\n","# create a folder called saved_results if it does not exist\n","if not os.path.exists('saved_results'):\n","    os.mkdir('saved_results')\n","    print('saved_results folder created')\n","else:\n","    print('saved_results folder already exists')"]},{"cell_type":"markdown","metadata":{"id":"wHhMpKpR0T5z"},"source":["# 2. Dataset"]},{"cell_type":"markdown","metadata":{"id":"Cfowo7xY-0B4"},"source":["# 2.1 Load and prepare Disaster Tweets dataset\n","Description: This dataset contains 7613 tweets that were hand classified as disaster or not disaster. The dataset is unbalanced, with only 3271 non-disaster tweets and 4342 disaster tweets. The goal is to predict which tweets are about real disasters and which one’s aren’t."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgJtqGLZKQn6"},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv('data/train.csv')\n","df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ayf0sdAw0T51"},"outputs":[],"source":["# drop unnecessary columns\n","df.drop(['id','keyword','location'],axis=1,inplace=True)\n","df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3KynZImKQn8"},"outputs":[],"source":["# print if there are any missing values\n","print('There are {} missing values in the dataset'.format(df.isnull().sum().sum()))\n","\n","# print the number of rows and columns in the dataset\n","print('There are {} rows and {} columns in the dataset'.format(df.shape[0],df.shape[1]))\n","\n","# print the names of the columns\n","print('The names of the columns are: {}'.format(df.columns))\n","\n","# print the number of tweets per class\n","print('The number of tweets per class are: ')\n","print('{}'.format(df.target.value_counts()))"]},{"cell_type":"markdown","metadata":{"id":"yZijARSX0T52"},"source":["## 2.2 Resampling\n","Disclaimer: use this code to resample the dataset if you want to use a balanced dataset. Otherwise, skip this cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"by788kXg0T52"},"outputs":[],"source":["from sklearn.utils import resample\n","\n","# Separate majority and minority classes\n","df_majority = df[df.target==0]\n","df_minority = df[df.target==1]\n","\n","# Downsample majority class\n","df_majority_downsampled = resample(df_majority, \n","                                   replace=False,    # sample without replacement\n","                                   n_samples=len(df_minority),     # to match minority class\n","                                   random_state=123) # reproducible results\n","\n","# Combine minority class with downsampled majority class\n","df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n","\n","df = df_downsampled.copy()\n","\n","# reset the index\n","df.reset_index(drop=True,inplace=True)\n","\n","# print the number of tweets per class\n","print('The number of tweets per class are: ')\n","print('{}'.format(df.target.value_counts()))"]},{"cell_type":"markdown","metadata":{"id":"x7MDpdDsKPg1"},"source":["# 3. Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"1jEmsylq0T53"},"source":["# 3.1 Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_1gOfKsKPg2"},"outputs":[],"source":["# apply preprocess_data function to the text column\n","df['clean_text'] = df['text'].apply(lambda x: preprocess_data(x))\n","df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQfHKW8XKPg3"},"outputs":[],"source":["# compare the original text with the clean text print the first 3 rows\n","print(df[['text','clean_text']].head(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbJsaZILKPg4"},"outputs":[],"source":["# Split the data into train and test sets\n","\n","df = df[['clean_text','target']]\n","\n","# rename the column clean_text to text\n","df.rename(columns={'clean_text':'text'},inplace=True)\n","\n","\n","# split the data into train and validation set keeping the dataframe structure with text and target\n","\n","df_train, df_test = train_test_split(df, test_size=0.2, random_state=16)\n","\n","# print the shape of the train and validation set\n","\n","print('There are {} rows and {} columns in train'.format(df_train.shape[0],df_train.shape[1]))\n","print('There are {} rows and {} columns in validation'.format(df_test.shape[0],df_test.shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"Dyqdlb2t0T55"},"source":["## 3.2 EDA: Easy Data Augmentation\n","\n","In the following section, we will use the textattack library to augment our training data. We will use the EDA (Easy Data Augmentation) technique to augment our training data. The EDA technique is a simple yet effective data augmentation technique that can be used to increase the size of our training data. The technique is based on the following steps:\n","- Randomly choose n words from the sentence.\n","- Synonym replacement: Replace each of the n words with one of its synonyms chosen at random.\n","- Random insertion: Insert n random words at random positions in the sentence.\n","- Random swap: Randomly swap pairs of words in the sentence n times.\n","\n","### Methodology\n","\n","We will apply the EDA technique to our training data. We will use the following parameters:\n","- n: 4\n","- alpha: 0.1\n","\n","From each sentence, we will generate 4 augmented sentences. We will use the alpha parameter to control the number of words that will be replaced, inserted or swapped. The alpha parameter is a float value between 0 and 1. The higher the value of alpha, the more words will be replaced, inserted or swapped.\n","\n","### Disclaimer: Data Augmentation\n","You use the following cells if you want to perform the classification task applying the EDA technique. If you want to use just the original dataset, you can skip the following cells.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38QyPfnq0T56"},"outputs":[],"source":["# print shape of train and test\n","\n","print('There are {} rows and {} columns in train'.format(df_train.shape[0],df_train.shape[1]))\n","print('There are {} rows and {} columns in validation'.format(df_test.shape[0],df_test.shape[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIpCY1s20T56"},"outputs":[],"source":["# perform data augmentation on the train set\n","df_train = augment_data(df_train, pct_words_to_swap=0.1, transformations_per_example=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xT1N_24x0T56"},"outputs":[],"source":["print('There are {} rows and {} columns in train'.format(df_train.shape[0],df_train.shape[1]))\n","print('There are {} rows and {} columns in validation'.format(df_test.shape[0],df_test.shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"8TyDvpLoKPg5"},"source":["# 3.3 Tokenization and Padding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgzuuyru0T57"},"outputs":[],"source":["# create x_train, y_train, x_val, y_val\n","\n","x_train, y_train = df_train['text'], df_train['target']\n","x_test, y_test = df_test['text'], df_test['target']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiF8XvXqKPg6"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n","tokenizer = Tokenizer()\n","\n","tokenizer.fit_on_texts(x_train)\n","word_index = tokenizer.word_index\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","print(\"Vocabulary Size :\", vocab_size)\n","\n","max_length = 30\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","# The tokens are converted into sequences and then passed to the pad_sequences() function\n","x_train = pad_sequences(tokenizer.texts_to_sequences(x_train),maxlen = max_length)\n","x_test = pad_sequences(tokenizer.texts_to_sequences(x_test),maxlen = max_length)\n","\n","# print shapes\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"y6qb7HEUKPg7"},"source":["# 3.4 GloVe Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPj1OwLpKPg7"},"outputs":[],"source":["embeddings_index = {}\n","# opening the downloaded glove embeddings file\n","embedding_dimension = 300\n","f = open('data/glove.6B.300d.txt', encoding='utf-8')\n","for line in f:\n","    # For each line file, the words are split and stored in a list\n","    values = line.split()\n","    word = value = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' %len(embeddings_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C_0LwZEKPg8"},"outputs":[],"source":["# creating an matrix with zeroes of shape vocab x embedding dimension\n","embedding_matrix = np.zeros((vocab_size, embedding_dimension))\n","# Iterate through word, index in the dictionary\n","for word, i in word_index.items():\n","    # extract the corresponding vector for the vocab indice of same word\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Storing it in a matrix\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"markdown","metadata":{"id":"ssz1JcAWKPg9"},"source":["# 4. RNN with LSTM cells"]},{"cell_type":"markdown","metadata":{"id":"oI5FGAZt0T59"},"source":["## 4.1 Define the RNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXKSrJSTKPg9"},"outputs":[],"source":["# create a folder called Glove_LSTM inside the saved_results folder if it does not exist\n","\n","if not os.path.exists('saved_results/Glove_LSTM'):\n","    os.mkdir('saved_results/Glove_LSTM')\n","    print('saved_results/Glove_LSTM folder created')\n","else:\n","    print('saved_results/Glove_LSTM folder already exists')\n","\n","\n","# create a folder AUG_Glove_LSTM inside the saved_results/Glove_LSTM folder if it does not exist\n","\n","if not os.path.exists('saved_results/Glove_LSTM/AUG_Glove_LSTM'):\n","    os.mkdir('saved_results/Glove_LSTM/AUG_Glove_LSTM')\n","    print('saved_results/Glove_LSTM/AUG_Glove_LSTM folder created')\n","\n","else:\n","    print('saved_results/Glove_LSTM/AUG_Glove_LSTM folder already exists')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VwMmUthKPg-"},"outputs":[],"source":["import tensorflow as tf\n","embedding_layer = tf.keras.layers.Embedding(vocab_size,embedding_dimension,\n","                                            weights=[embedding_matrix],\n","                                          input_length=max_length,trainable=False)\n","\n","\n","# Import various layers needed for the architecture from keras\n","from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\n","from tensorflow.keras.layers import SpatialDropout1D\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# The Input layer \n","sequence_input = Input(shape=(max_length,), dtype='int32')\n","# Inputs passed to the embedding layer\n","embedding_sequences = embedding_layer(sequence_input)\n","# Passed on to the bi-directional LSTM layer\n","x = Bidirectional(LSTM(64, return_sequences=True))(embedding_sequences)\n","x = Dropout(0.5)(x)\n","# Passed on to the second bi-directional LSTM layer\n","x = Bidirectional(LSTM(32))(x)\n","x = Dropout(0.5)(x)\n","# Passed on to dense layer with ReLU activation\n","x = Dense(20, activation='relu')(x)\n","# Passed on to sigmoid output layer\n","outputs = Dense(1, activation='sigmoid')(x)\n","model = tf.keras.Model(sequence_input, outputs)\n","\n","# Compile the model with binary crossentropy loss function and Adam optimizer\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Use early stopping with a patience of 3 epochs\n","from tensorflow.keras.callbacks import EarlyStopping\n","early_stopping = EarlyStopping(patience=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjaDK4P80T5-"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"8MFx4m3N0T5_"},"source":["## 4.2 Train and evaluate the RNN model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRjFM08vKPg_"},"outputs":[],"source":["# Train the model\n","training = model.fit(x_train, y_train, batch_size=1024, epochs=50,\n","                    validation_data=(x_test, y_test), callbacks=[early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqZe92xYKPhA"},"outputs":[],"source":["from utils import plot_and_save_loss\n","from utils import plot_and_save_accuracy\n","\n","# Plotting the training and validation loss\n","\n","plot_and_save_loss(training, 'saved_results/Glove_LSTM/loss.png') \n","# plot_and_save_loss(training, 'saved_results/Glove_LSTM/AUG_Glove_LSTM/loss.png')\n","\n","# Plotting the training and validation accuracy\n","\n","plot_and_save_accuracy(training, 'saved_results/Glove_LSTM/accuracy.png')\n","# plot_and_save_accuracy(training, 'saved_results/Glove_LSTM/AUG_Glove_LSTM/accuracy.png')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhD8XTU_KPhA"},"outputs":[],"source":["# Predicting the test data\n","y_pred = model.predict(x_test)\n","\n","# Converting the predicted values to 0 or 1\n","y_pred = np.round(y_pred)\n","\n","print_metrics(y_test, y_pred)\n","\n","plot_confusion_matrix(y_test, y_pred, 'saved_results/Glove_LSTM/Glove_LSTM_Confusion_Matrix.png')\n","# plot_confusion_matrix(y_test, y_pred, 'saved_results/Glove_LSTM/AUG_Glove_LSTM/Glove_LSTM_Confusion_Matrix.png')\n","\n","\n","save_metrics(y_test, y_pred, 'saved_results/Glove_LSTM')\n","# save_metrics(y_test, y_pred, 'saved_results/Glove_LSTM/AUG_Glove_LSTM')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2 (tags/v3.8.2:7b3ab59, Feb 25 2020, 23:03:10) [MSC v.1916 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a1747d3b2025a168ac2f833bd83bf6cc601268c3e1826b5adb5d5d8dda22ff57"}}},"nbformat":4,"nbformat_minor":0}
